## Which way to search
We can search for companies by "Entity Name" or by "File Number".

According to `this random webpage <http://blog.delawareinc.com/delaware-state-file-number/>`_,

> The Delaware state file number is the number the state of Delaware issues to each new company at the time of formation. It is formatted with seven numbers. For example, 1234567. Each new Delaware company will be one digit higher than the company before it. It is the number that you can use to identify your company when dealing with the Delaware secretary of state.

Thus, I think "File Number" is a decent way to look up companies.



## Architecture
My inclination is to use a worker-manager architecture, but maybe I should
use something less classist? Peer-to-peer connections are annoying because
of port blocking of various sorts, but that would be nice because then I
don't need to be responsible.

I think the process will be quite similar if I use peers to if I use a
single manager, so it's not that big a deal, actually.

### Deciding jobs
The worker contacts the manager asking for a job.

    POST https://$username:$installationid@deleware.dada.pink/directions

We use ``POST`` to avoid cache issues.

The ``$username`` is chosen by the user so she can get points for her efforts.
The ``$installationid`` is a UUID chosen when the program is first run. It acts
as a password; this way, if someone tries to submit fake results under another
person's username, we can separate the real person's results from the other
person's results.

In response, the worker will receive either a status code of 429
(too many requests) or a status code of 200. The manager decides which one
based on how many requests have come from this IP address recently. It

If the manager provides a status code of 200, it also provides the following
information.

* a file number to look up
* an IP address, called the "before" address.

The IP address is the worker's own IP address, and the manager can figure that
out pretty easily.

The file number is chosen randomly from the list of file numbers. The sampling
(not normalized) weight is equal to two to the negative power of the number
of responses for this file number so far.

> 4 ^ (- responses.so.far)

For example, everything has equal weights (1) when we start because there have
been zero responses so far. Then when things get selected the first time, they
have lower weights (1/4). This way, we can be intelligent about which file numbers
we try without assigning jobs to particular workers.


### Running jobs
Once the bot has been directed to look up a particular file number, it queries
the Deleware corporations site accordingly. It goes to the starting page for
the General Information Name Search (called ``home`` in the code). It enters
the file number and receives a list of up to one company. (This page is called
a ``search`` in the code.) It then goes to this maybe-company page (called
``result`` in the code).

At every step, the bot

* minimally parses the web page so that it may advance to the next step,
* sends simplified HTTP response information to the manager
* pauses randomly for a time on the order of a second to avoid looking so obviously like a bot

The HTTP responses look like this. ::

    POST https://$username:$installationid@deleware.dada.pink/response
    before_ip_address: 123.456.67.89
    body is the serialized request

### Saving information on the manager
When the manager recieves a response, it first needs to determine an
additional piece of information. The worker has provided the "before"
IP address; the manager now determines the "after" IP address.

Having determined this, it writes the following stuff to a simple log file.

* username
* installation id
* before ip address
* after ip address
* serialized request

It also saves the IP address(es) in an IP address table. We maintain this
table so we can avoid exceeding thresholds for IP blocking. If the before
and after IP addresses are different, we conservatively count the request
as having come from both addresses.

Finally, it parses the file number from the response and updates the
sampling weights for the file number selection.

A separate process comes along later, reads the log files, and reads more
information from the response. The involved parsing is moved to a separate
task for two main reasons. First, this reduces the load of the manager.
Second, we can reuse the separate task for loading backups; we don't need
to write a separate thing for that.

### Waiting
The worker waits a random time on the order of minutes or hours before
repeating the above process. This way, the bots may look a bit less like
bots and thus be harder to block.

## Questions you might have
Why not just in-browser Javascript?
    We can't make cross-domain requests, so we'd have to inject something into the Deleware page, and that's annoying, especially for this site.

Doesn't OpenCorporates already have it?
    OpenCorporates doesn't have it.

Have people done similar things in terms of this distibuted API?
    Probably

Why Python rather than something that people with Windows can run?
    Because it's easier
